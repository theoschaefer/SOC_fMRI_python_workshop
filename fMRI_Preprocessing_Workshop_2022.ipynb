{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fMRI preprocessing - hands-on workshop\n",
    "\n",
    "#### Max Planck School of Cognition, Methods in Cognitive Neuroscience, July 2022\n",
    "\n",
    "\n",
    "##### Contacts: \n",
    "**Frauke Beyer** @ Neurology Dept., MPI CBS, Leipzig, Germany, fbeyer@cbs.mpg.de  \n",
    "**Alexander Nitsch** @ Psychology Dept., MPI CBS, Leipzig, Germany, nitsch@cbs.mpg.de  \n",
    "**Theo Sch√§fer** @ Psychology Dept., MPI CBS, Leipzig, Germany, tschaefer@cbs.mpg.de\n",
    "\n",
    "Please contact us if you have any questions or problems with the notebook! Happy to help :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "Preprocessing of functional MRI data is an important step in analyzing both task-based and resting state fMRI studies.  \n",
    "In this notebook, we will cover the main steps of fMRI preprocessing. You will\n",
    "* learn how to plot and inspect fMRI data in python\n",
    "* implement main preprocessing steps for fMRI data\n",
    "* find out how to use `nipype` to embed different preprocessing steps\n",
    "\n",
    "fMRI preprocessing can be performed by several software packages (e.g. SPM, FSL, AFNI) or standardized pipelines such as [fMRIPrep](https://fmriprep.org/en/stable/), [C-PAC](https://fcp-indi.github.io/docs/latest/user/quick.html) or [ABCD-Pipeline](https://github.com/DCAN-Labs/abcd-hcp-pipeline). The purpose of this tutorial is to familiarize yourself with each preprocessing step in detail so that you get a feeling of how these influence / transform the data. \n",
    "\n",
    "Additional useful resources on fMRI preprocessing / analysis:\n",
    "\n",
    "- Lecture series: Principles of fMRI, including preprocessing [part 1](https://www.youtube.com/watch?v=Qc3rRaJWOc4&list=PLfXA4opIOVrGHncHRxI3Qa5GeCSudwmxM&index=16) and [part 2](https://www.youtube.com/watch?v=qamRGWSC-6g&list=PLfXA4opIOVrGHncHRxI3Qa5GeCSudwmxM&index=17)\n",
    "- DartBrains Tutorial on fMRI analysis, including [preprocessing](https://dartbrains.org/content/Preprocessing.html)\n",
    "- NI-edu by Lukas Snoek, tutorial on fMRI analysis, including [preprocessing](https://lukas-snoek.com/NI-edu/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages\n",
    "\n",
    "We will use one software package and different python packages in this hands-on session:\n",
    "* <a href='fsl.fmrib.ox.ac.uk/'>FSL</a> is an open-source neuroimaging toolbox which is very common and extremely versatile. In FSL, you can do  task-based and resting state functional MRI analyses, as well as structural and diffusion MRI. You can find an overview of its functions and tutorials on the homepage. \n",
    "\n",
    "* [nipype](https://nipype.readthedocs.io/) is a Python project that provides a uniform interface to existing neuroimaging software and facilitates interaction between these packages within a single workflow. With Nipype you can integrate algorithms from different packages (e.g., ANTS, SPM, FSL, FreeSurfer, Camino, MRtrix, MNE, AFNI, Slicer, DIPY), into a single workflow which has many advantages in terms of flexibility, run time, reproducibility... \n",
    "\n",
    "* [nibabel](https://nipy.org/nibabel/) is a Python project which enables the handling of NIFTI-files, the most common format for MRI data, in python\n",
    "\n",
    "* [nilearn](https://nilearn.github.io/) is a package for fast and easy statistical learning and plotting for NeuroImaging data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "In the hands-on sessions, we will primarily use functional and T1-weighted MRI data from `sub-160563d` located in `./fMRI_data`. The subject participated in the LIFE-Upgrade study at the MPI CBS and agreed on sharing his/her data. The MRI dataset is included in the repository within the `fMRI_data` folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages\n",
    "#### FSL\n",
    "[FSL](https://www.fmrib.ox.ac.uk/fsl) is a neuroimaging software package which includes many functions useful for preprocessing and analysis of fMRI.  \n",
    "It has been installed for us on the jupyter-hub in a public folder. We have to add the path of the FSL binaries to our environment path and set the FSL directory. This step has to be performed every time we use the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# set FSL paths\n",
    "os.environ[\"PATH\"] += os.pathsep + \"/home/\" + getpass.getuser() + \"/public/0579223/MCNcourse2022/fsl/bin\"\n",
    "os.environ[\"FSLDIR\"] = \"/home/\" + getpass.getuser() + \"/public/0579223/MCNcourse2022/fsl/\"\n",
    "os.environ[\"FSLOUTPUTTYPE\"]=\"NIFTI_GZ\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run commands in the shell from your notebook, use a \"!\" in front of the command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test whether your FSL installation worked by typing\n",
    "!echo $FSLDIR # should give you the location of your installation\n",
    "!mcflirt # should return description of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages\n",
    "#### python-based packages\n",
    "Now, we install `nipype 1.6.0`, `nibabel 2.5.2`, `nilearn 0.8.0`, `nipy 0.5.0`, `nitime 0.9` and `joblib 1.0.1` via `!pip install --user ...`. Again, we perform the installation with the ! directly from our notebook. \n",
    "The installation has to be done only once (so if you installed the packages during the EEG tutorial, you do not need to re-install them). Loading the packages has to be performed everytime you use the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user nipype==1.6.0\n",
    "!pip install --user nibabel\n",
    "!pip install -U --user nilearn==0.8.0\n",
    "!pip install -U --user joblib==1.0.1\n",
    "!pip install --user nitime==0.9\n",
    "!pip install nipy==0.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import these packages, along with numpy and scipy and check their versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case packages can't be loaded\n",
    "#os.environ[\"PATH\"] += \"/home/jovyan/.local/lib/python3.9/site-packages/\"\n",
    "\n",
    "import nipype\n",
    "print(f\"nipype version {nipype.__version__}\")\n",
    "import nibabel as nib\n",
    "print(f\"nibabel version {nib.__version__}\")\n",
    "import nilearn\n",
    "print(f\"nilearn version {nilearn.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(f\"numpy version {np.__version__}\")\n",
    "import scipy\n",
    "print(f\"scipy version {scipy.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set the paths to the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to functional and anatomical directories\n",
    "dir_func = \"/home/\" + getpass.getuser() + \"/SOC_fMRI_python_workshop/fMRI_data/sub-160563d/func/\"\n",
    "dir_anat = \"/home/\" + getpass.getuser() + \"/SOC_fMRI_python_workshop/fMRI_data/sub-160563d/anat/\"\n",
    "\n",
    "# Path to functional image\n",
    "func = dir_func + \"sub-160563d_task-rest_bold.nii.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "First, we load the `matplotlib` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to run interactive matplotlib plots in jupyter hub, we have to set the backend of the matplotlib package to notebook. This can be achieved by `%matplotlib notebook`.  \n",
    "Note: If you are running the notebook on your local machine, the backend tk is prefered. You can set it by `%matplotlib tk`.  \n",
    "In order to go back to inline plotting, you can write `%matplotlib inline`.\n",
    "\n",
    "For more tips & tricks on how to best plot interactively in jupyter notebooks, [this](https://medium.com/@1522933668924/using-matplotlib-in-jupyter-notebooks-comparing-methods-and-some-tips-python-c38e85b40ba1) page  is quite helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "To load the data, we use nibabel's `load` function. This function converts the data from the [NIFTI-format]() into a `Nifti1Image` python object.   \n",
    "\n",
    "A nibabel `Nifti1Image` object is the association of three things:  \n",
    "* an numpy N-D array containing the image data;\n",
    "* a (4, 4) affine matrix which specifies the orientation of the image\n",
    "* image metadata in the form of a header.\n",
    "\n",
    "Many functions from python neuroimaging packages such as `nilearn` and `nipy` directly operate on the `Nifti1Image`. Yet, you can also extract the `numpy` array from the object and directly modify this array.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The affine matrix of the nibabel `Nifti1Image` maps the numpy-array to the anatomical or patient coordinate system. This coordinate system is orthogonal and oriented along the main axes of the participant. In a RAS+ coordinate system, the X (or first dimension of the numpy array) describes the **r**ight->left axis, Y refers to **a**nterior-> posterior, and Z **s**uperior-inferior. For more information on imaging coordinate systems, please see [here](https://www.slicer.org/wiki/Coordinate_systems)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the nifti of the functional scan into a nibabel image object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_func = nib.load(func)\n",
    "\n",
    "# alternatively with nilearn\n",
    "#img_func = nilearn.image.load_img(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Exercise 1\n",
    "Find out about the image dimensions and total duration of this scan. You can use the built-in methods\n",
    "(or functions) of the nibabel image-object `.shape` and `.header` described [here](https://nipy.org/nibabel/gettingstarted.html) and the nibabel methods for reading the image header [here](https://nipy.org/nibabel/nibabel_images.html). Advanced: Where would you find the TR (2s)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Visualizing the data is always very helpful to get a feeling for the data and notice mistakes. As you saw in the exercise above, the fMRI data has several hundred volumes, i.e. imaging timepoints. This means we can either go through volumes over time, display one out of these volumes, or show the mean image.   \n",
    "Going through the fMRI data over time is most easily done in 3D visualization software such as `fsleyes`, `mricron` or `freeview`.  \n",
    "Unfortunately, these visualization tools cannot be used in the jupyter notebook. Instead, we can use `nilearn` to view the brain images or simple `matplotlib` plotting commands to scroll through the images interactively. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use nilearn to plot the mean image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the mean image\n",
    "# We do this by taking the mean along the 4th dimension (time)\n",
    "from nilearn.image.image import mean_img\n",
    "mean_func = mean_img(func)\n",
    "mean_func.to_filename('./fMRI_data/sub-160563d/func/mean_func_raw.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the fMRI in orthogonal slices (with nilearn)\n",
    "from nilearn.plotting import plot_epi\n",
    "plot_epi(mean_func, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use matplotlib to plot any slice of the 4D image array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the fMRI (with numpy array & matplotlib)\n",
    "\n",
    "# get image data\n",
    "img_func_data = img_func.get_fdata()\n",
    "\n",
    "# We have to select 2 dimensions from the 4D data set to be able to plot it in 2D with imshow.\n",
    "\n",
    "# With this command, we plot the 21st axial slice of the second timepoint!!!\n",
    "# Remember that python starts to enumerate from 0!\n",
    "plt.imshow(img_func_data[:,:,20,1])\n",
    "plt.show()\n",
    "\n",
    "# with gray colormap this looks more like a brain: \n",
    "plt.imshow(img_func_data[:,:,20,1], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a function to interactively plot the 4D fMRI data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the package `ipywidgets` to create sliders which allow us to interactively modify a function. Here, we first define the function `plot_slice` to plot a single slice of the image (in the z, so inferior/superior, direction and at a single timepoint). Then, we create a second function `z_vol_viewer` in which we can interactively modify the slice and the timepoint with a slider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_slice(volume,z,vol):\n",
    "    plt.imshow(volume[:,:,z,vol].T, cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "def z_vol_viewer(img):\n",
    "    interact(plot_slice, volume=fixed(img), \n",
    "             z = widgets.IntSlider(value=img.shape[2]//2,\n",
    "                                   min=0,\n",
    "                                   max=img.shape[2]-1,\n",
    "                                   step=1,\n",
    "                                   description=\"z-axis\"),\n",
    "             vol = widgets.IntSlider(value=0,\n",
    "                                     min=0,\n",
    "                                     max=img.shape[3]-1,\n",
    "                                     step=1,\n",
    "                                     description=\"volume\"),\n",
    "            )\n",
    "\n",
    "z_vol_viewer(img_func_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 \n",
    "If you feel like it, you can try to create a truely 3D/4D viewer by copying and then extending the function `z_vol_viewer` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_slice(volume,axis,slicenum,vol):\n",
    "    if axis=='x':\n",
    "        plt.imshow(volume[slicenum,:,:,vol].T, cmap=\"gray\", origin=\"lower\")\n",
    "    elif axis=='y':\n",
    "        plt.imshow(volume[:,slicenum,:,vol].T, cmap=\"gray\", origin=\"lower\")\n",
    "    elif axis=='z':\n",
    "        plt.imshow(volume[:,:,slicenum,vol].T, cmap=\"gray\", origin=\"lower\")\n",
    "    plt.show()\n",
    "\n",
    "def vol_viewer(img, axis):\n",
    "    \n",
    "    if axis=='x': ax=0\n",
    "    elif axis=='y': ax=1\n",
    "    elif axis=='z': ax=2\n",
    "    \n",
    "    interact(plot_slice, axis=fixed(axis), volume=fixed(img), \n",
    "             slicenum = widgets.IntSlider(value=img.shape[ax]//2,\n",
    "                                   min=0,\n",
    "                                   max=img.shape[ax]-1,\n",
    "                                   step=1,\n",
    "                                   description=\"slice\"),\n",
    "             vol = widgets.IntSlider(value=0,\n",
    "                                     min=0,\n",
    "                                     max=img.shape[3]-1,\n",
    "                                     step=1,\n",
    "                                     description=\"volume\"),\n",
    "            )\n",
    "\n",
    "vol_viewer(img_func_data, 'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's inspect the time dimension of the fMRI data. To do so, we will plot the raw time series of two voxels (selected from the \"middle\" of the brain based on the plots above) by subsetting the `img_func_data` Numpy array. Try to play around with the voxel locations and compare the signal intensity and time courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(img_func_data[32,58,15, :])\n",
    "plt.plot(img_func_data[24,30,10, :])\n",
    "\n",
    "# plot multiple x locations from list\n",
    "#[plt.plot(img_func_data[x,58,15, :]) for x in [22, 24, 32, 50]]\n",
    "\n",
    "plt.xlabel('Time [TRs]', fontsize=16)\n",
    "plt.ylabel('Intensity', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing   \n",
    "Now that we have inspected our data, we can start preprocessing it. This means we prepare the data to be statistically analyzed by removing confounds and error variance.\n",
    "\n",
    "fMRI preprocessing involves many steps, and research groups tend to do it \"their own way\" - by using specific software packages and algorithms, leaving out certain steps or choosing different parameters. This contributes to the relatively low reproducibility of neuroimaging research (see [Nature paper](https://www.nature.com/articles/s41586-020-2314-9) on the [NARPS](https://www.narps.info/) project). \n",
    "Here, we would like to show you the \"core\" preprocessing steps that are largely agreed upon in the community (and implemented in [fMRIprep](https://www.nature.com/articles/s41592-018-0235-4?proof=trueIn), a project aiming to standardize fMRI preprocessing).\n",
    "\n",
    "### Preprocessing steps\n",
    "1. realignment of volumes to reference volume and extraction of motion parameters\n",
    "2. skull stripping\n",
    "- *slice time correction (used only for high TR and single-band acquisition)*\n",
    "- *susceptibility distortion correction (recommended but not covered here)*\n",
    "3. Spatial normalization\n",
    "4. Denoising (removal of physiological and head motion-related noise)\n",
    "5. Temporal filtering (removal of linear trend)\n",
    "6. Smoothing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration of different software packages with nipype\n",
    "To perform the preprocessing steps, we have a large variety of neuroimaging software packages (FSL, Freesurfer, AFNI, SPM, nipy, ANTS ...) at our disposal. However, combining software from different packages in a clever and reproducible way is a big challenge.   \n",
    "Here, `nipype` comes very handy: this python package provides a uniform interface to existing neuroimaging software and allows us to efficiently combine different software packages, dedicated python packages and our own functions. At first, it may seem a bit unintuitive to work with this 'overlay', yet, later it makes your life much easier by offering structure, adaptibility and scalability. Another advantage of learning `nipype` is that `fmriprep` and other recent software developments for reproducible neuroscience relie on its concepts. \n",
    "\n",
    "**nipype in a nutshell**  \n",
    "[workflows](https://miykael.github.io/nipype_tutorial/notebooks/basic_workflow.html) are the main feature of `nipype`. Workflows allow the intergration of different functions (from neuroimaging toolboxes or self-written) and the efficient computational execution of complex pipelines.  \n",
    "[nodes](https://miykael.github.io/nipype_tutorial/notebooks/basic_nodes.html) are the building blocks of workflows and objects which execute a certain function. This function can be anything from a Nipype interface (wrapping an external algorithm) to a user-specified function or an external script.   \n",
    "[interfaces](https://miykael.github.io/nipype_tutorial/notebooks/basic_interfaces.html) wrap a single command from various external packages (e.g. FSL, SPM or FreeSurfer), even if they themselves are written in another programming language than python. \n",
    "In the following, you will get to know the some of these concepts, but for a more complete introduction and explanation, please see the links to the great tutorials by [mykael](https://miykael.github.io/nipype_tutorial/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![picture](https://miykael.github.io/nipype-beginner-s-guide/_images/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Realignment \n",
    "During the fMRI scan, participants frequently move their heads. This head motion can either be a slight drift, i.e. lowering the chin to the chest, or single abrupt head movements, for example when the subject awakes from a brief nap. Look at the motion in the image below.\n",
    "\n",
    "![picture](https://miykael.github.io/nipype-beginner-s-guide/_images/movement.gif)\n",
    "\n",
    "Therefore, the first preprocessing step is the realignment or motion correction of the scan. This operation aims to coregister all functional volumes to a common reference (oftentimes the first or middle volume), thereby aligning each voxel with itself over time. \n",
    "Here, we use the `MCFLIRT` [program](https://nipype.readthedocs.io/en/1.5.0/api/generated/nipype.interfaces.fsl.preprocess.html#mcflirt) from FSL for motion correction.  \n",
    "First, we execute this command directly with `bash` (this can take a few minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use bash to run MCFLIRT (specify in- & output)\n",
    "!mcflirt -in ./fMRI_data/sub-160563d/func/sub-160563d_task-rest_bold.nii.gz -out ./fMRI_data/sub-160563d/func/sub-160563d_task-rest_bold_mcf.nii.gz -plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: which volume is used as reference volume by MCFLIRT and how can you change this parameter?\n",
    "Hint: Check the manual of the command MCFLIRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mcflirt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use  `nipype` to wrap the command in an interface. First, we load the interface for FSL from `nipype.interfaces.fsl`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load nipype interface for FSL\n",
    "import nipype.interfaces.fsl as fsl\n",
    "fsl.FSLCommand.set_default_output_type('NIFTI_GZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the input (`inputs.in_file`) and options (`save_plots=True`) for the interface.   \n",
    "We can see the commandline which is executed with the command `run` by printing `mcflrt.cmdline`. We see that it is essentially the same command as we wrote in bash before. \n",
    "The result ist saved in a python results-object, and can be assessed via `outputs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run spatial realignment \n",
    "mcflirt = fsl.MCFLIRT(save_plots=True) #define the interface and tell it to save plots\n",
    "mcflirt.inputs.in_file = func #define input to the interface\n",
    "mcflirt.inputs.out_file = './fMRI_data/sub-160563d/func/mcflirt.nii.gz'\n",
    "print(mcflirt.cmdline) #show the commandline which is created by this interface\n",
    "mcflirt_res = mcflirt.run() #run the interface, i.e. command line\n",
    "par_file = mcflirt_res.outputs.par_file #extract the parameter file from the result object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mcflirt_res.outputs) #show the outputs of this node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A Comment\n",
    "<font color='gray'>\n",
    "\n",
    "If you have completed the larger preprocessing steps once (and the outputs are located in `/fMRI_data/sub-160563d/func`), you don't have to run these steps again and instead refer to the full paths of the outputs in the upcomping preprocessing steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Skull stripping\n",
    "\n",
    "Until now, our fMRI data still includes skull and neck areas. For further processing, we would like to extract the signal coming exclusively from the brain. Therefore, we have to perform skull stripping and create a brain mask. To perform skull stripping, we need a reference image. Here, we take the mean image of the motion corrected fMRI data as reference image.    \n",
    "\n",
    "### Create the mean image of motion corrected fMRI data\n",
    "To calculate the mean of the motion corrected fMRI data, we could do it as above and re-use the nilearn function `mean_img`. But we would like to get into `nipype` and so we employ a `FSL` tool wrapped in `nipype`.   \n",
    "The interface `ImageMaths` wraps the FSL command `fslmaths`, a versatile tool to perform voxel-wise calculations. The option `Tmean` calculates the 4D mean of the fMRI data.  \n",
    "Below, we define the interface and define its input (`in_file`), output name (`out_file`) and the `Tmean` option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean motion corrected fMRI\n",
    "mean_mcflirt_interface = fsl.ImageMaths(in_file = \"./fMRI_data/sub-160563d/func/mcflirt.nii.gz\", #mcflrt_res.outputs.out_file, #use output of previous interface as input\n",
    "                                        out_file = \"./fMRI_data/sub-160563d/func/mcflirt_mean_interface.nii.gz\",\n",
    "                                        op_string = '-Tmean')\n",
    "res_mean_mcflirt_interface = mean_mcflirt_interface.run()\n",
    "\n",
    "# or with nilearn\n",
    "#mean_mcflirt_nilearn = mean_img(\"./fMRI_data/sub-160563d/func/mcflirt.nii.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same command can be wrapped with a Nipype `node`. This is an extension of the interface concept, making it possible to combine more operations in a workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Node module\n",
    "from nipype import Node\n",
    "\n",
    "# Create Node\n",
    "mean_mcflirt_node = Node(fsl.ImageMaths(), name='mean_mcflirt_node')\n",
    "mean_mcflirt_node.inputs.in_file = dir_func + 'mcflirt.nii.gz'\n",
    "mean_mcflirt_node.inputs.out_file = dir_func + 'mcflirt_mean_node.nii.gz'\n",
    "mean_mcflirt_node.inputs.op_string = \"-Tmean\"\n",
    "res_mean_mcflirt_node = mean_mcflirt_node.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the two (identical) results using `plot_epi`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you ran the commands above:\n",
    "plot_epi(res_mean_mcflirt_interface.outputs.out_file)\n",
    "plot_epi(res_mean_mcflirt_node.outputs.out_file)\n",
    "\n",
    "#if you ran the commands before and have pre-computed data:\n",
    "#plot_epi(\"./fMRI_data/sub-160563d/func/mcflirt_mean_interface.nii.gz\")\n",
    "#plot_epi(\"./fMRI_data/sub-160563d/func/mcflirt_mean_node.nii.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the skull stripping\n",
    "\n",
    "Skull stripping can be achieved with the function BET in FSL, wrapped in [nipype](https://nipype.readthedocs.io/en/1.5.0/api/generated/nipype.interfaces.fsl.preprocess.html#bet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bet = fsl.BET(mask=True)\n",
    "bet.inputs.in_file = \"./fMRI_data/sub-160563d/func/mcflirt_mean_node.nii.gz\"  # res_mean.outputs.out_file\n",
    "bet.inputs.out_file = \"./fMRI_data/sub-160563d/func/mcflirt_mean_brain.nii.gz\"\n",
    "bet_res = bet.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bet_res.outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use the `compute_epi_mask` function in nilearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.masking import compute_epi_mask\n",
    "mask_img = compute_epi_mask(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the brain mask overlaid with the functional image\n",
    "from nilearn.plotting import plot_roi\n",
    "plot_roi(mask_img, res_mean_mcflirt_interface.outputs.out_file)\n",
    "plot_roi(bet_res.outputs.mask_file, res_mean_mcflirt_interface.outputs.out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mask created with BET looks slightly better as it does not exclude so many voxels in the medial OFC. On the other hand, it seems to include skull in some parts of the brain (e.g. near visual cortex)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spatial normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to bring all functional images into the same reference space to compare task-based activations or resting state ongoing activity. Usually, we do this by registering the functional and anatomical images of the same individual, aligning the individual anatomical images to a common template and then applying both operations to the fMRI scan. See the figure below for the three-step procedure:\n",
    "![\"Registration\"](registration_steps.png)\n",
    "As discussed in the lecture, there are some drawbacks of this method (e.g. participants may have very different anatomy or may differ in where activation occurs), yet it is still the standard in fMRI research.  \n",
    "\n",
    "Often, the MNI space is chosen as template space. Here, different versions of template are available, depending on the number and kinds of averages, and whether the template was constructed to be symmetric or asymmetric. See the [MNI ICBM page](http://nist.mni.mcgill.ca/?p=904) for a more detailed description of the available templates and the [fmriprep documentation](https://fmriprep.org/en/20.1.1/spaces.html?highlight=mni#standard-spaces) for all templates available in `fmriprep`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anatomical Data\n",
    "\n",
    "When we do a fMRI Study, we always acquire a T1-weighted anatomical sequence along with the functional scans. The T1-weighting of these images provides a good grey matter-white matter contrast and depiction of the anatomy in these images.\n",
    "\n",
    "We use the anatomical data to \n",
    "1. spatially coregister the functional scans of our study partipants together - due to the low contrast and anatomical detail, we cannot directly use the functional scan for it. \n",
    "2. specify regions of interest for functional imaging analysis (seed-based connectivity or regions of interest for activation studies) and denoising\n",
    "\n",
    "Common T1-weighted sequences are MPRAGE (Magnetization Prepared Rapid Gradient Echo) on Siemens or 3D TFE (Turbo Field Echo) on Philips scanners. \n",
    "The anatomical data (`T1w` from a MPRAGE sequence) is located in the `anat_MRI` folder in the `soc_coding_tutorials` repository.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to anatomical image\n",
    "T1w = dir_anat + \"sub-160563d_T1w.nii.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the T1W image. We can use the special nilearn function `plot_anat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "plotting.plot_anat(T1w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Co-registration of functional scan to structural T1w scan\n",
    "Before we perform the co-registration of functional and anatomical scan, we remove the skull and other non-brain tissue from the anatomical scan to achieve a better result.\n",
    "Like for the functional imaging, we can use `BET` to perform skull-stripping.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brain extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the brain from the T1w image with `BET`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bet_T1w = fsl.BET(mask=True)\n",
    "bet_T1w.inputs.in_file = T1w\n",
    "bet_T1w.inputs.out_file = dir_anat + \"T1w_brain.nii.gz\"\n",
    "res = bet_T1w.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the brain mask and check whether it is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_roi\n",
    "plot_roi(res.outputs.mask_file, T1w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brain extraction did not work perfectly. We see that a part of the neck and brainstem area is still included. We can give `BET` the coordinates of (roughly) the center of gravity (i.e. the middle of the brain) to improve estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bet_T1w = fsl.BET(mask=True)\n",
    "bet_T1w.inputs.in_file = T1w\n",
    "bet_T1w.inputs.center = [86,106,180] \n",
    "bet_T1w.inputs.out_file = dir_anat + \"T1w_brain_masked.nii.gz\"\n",
    "res = bet_T1w.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_roi\n",
    "plot_roi(res.outputs.mask_file,T1w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the brain mask looks fine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this skull-stripped brain to perform the registration with the functional image. For this linear registration, we use FLIRT. If the field-of-view is large (i.e. whole-brain fMRI acquisition), and no calibration has been performed on your scanner (which may cause scaling differences between acquisitions), 6 degrees-of-freedom (i.e. 3 translations + 3 rotations) are appropriate. More answers to frequently asked questions around FLIRT, see [here](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FLIRT/FAQ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use  `nipype` to wrap the command in a Node. First, we load the interface for FSL from `nipype.interfaces.fsl`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg = Node(fsl.FLIRT(dof=6, cost_func='mutualinfo'), name='linear_reg')\n",
    "linear_reg.inputs.in_file = dir_func + \"mcflirt_mean_brain.nii.gz\"\n",
    "linear_reg.inputs.reference = dir_anat + \"T1w_brain_masked.nii.gz\"\n",
    "linear_reg.inputs.out_file = dir_anat + \"fmri_to_t1w.nii.gz\"\n",
    "linear_reg.inputs.out_matrix_file = dir_anat + \"fmri_to_t1w.mat\"\n",
    "linear_reg_res=linear_reg.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the result. Unfortunately, the visualization is not ideal in the notebook. If possibly, download these files and have a look at them in `FSLEYES` or `FREEVIEW`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = plotting.plot_anat(dir_anat + \"T1w_brain_masked.nii.gz\", title=\"coregistration anat-fMRI\")\n",
    "display.add_edges(dir_anat + \"fmri_to_t1w.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_anat(dir_anat + \"fmri_to_t1w.nii.gz\", cut_coords=[-11, 13, -1])\n",
    "plotting.plot_anat(dir_anat + \"T1w_brain_masked.nii.gz\", cut_coords=[-11, 13, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "Compare the result of the registration with the alignment of the images before the registration (using the same plotting logic as above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Non-linear spatial registration of anatomical image and template "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we perform the second step, the non-linear registration of anatomical image and reference template.   \n",
    "First, we use `nilearn` to fetch the MNI ICBM 2009 template. We use the key `T1` to access the T1-weighted template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "icbm = datasets.fetch_icbm152_2009()\n",
    "template_ICBM = icbm['t1']\n",
    "template_ICBM_mask = icbm['mask']\n",
    "\n",
    "apply_MNI_mask = fsl.ImageMaths()\n",
    "apply_MNI_mask.inputs.in_file = template_ICBM\n",
    "apply_MNI_mask.inputs.mask_file = template_ICBM_mask\n",
    "apply_MNI_mask.inputs.out_file = \"/home/\" + getpass.getuser() +\"/SOC_fMRI_python_workshop/fMRI_data/ICBM_MNI_masked.nii.gz\"\n",
    "apply_MNI_mask.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start with the non-linear registration, we perform a \"pre\" registration step where we rigidly align the T1w image and the MNI template. (You can try and see what happens if you omit this step :))\n",
    "This step takes quite a while, so please be patient and wait. If you have run it once, you can specify the path to the result files in the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prereg = Node(fsl.FLIRT(dof=6, cost_func='mutualinfo'), name='prereg')\n",
    "prereg.inputs.reference = \"/home/\" + getpass.getuser() +\"/SOC_fMRI_python_workshop/fMRI_data/ICBM_MNI_masked.nii.gz\"\n",
    "prereg.inputs.in_file = dir_anat + \"T1w_brain.nii.gz\"\n",
    "prereg.inputs.out_file = dir_anat + \"t1w_2_mni_prereg.nii.gz\"\n",
    "prereg.inputs.out_matrix_file = dir_anat + \"t1w_2_mni_premat.mat\"\n",
    "prereg_res=prereg.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we give this pre-matrix to the non-linear registration function `FNIRT` to perform the non-linear registration of the anatomical image and the MNI space (i.e. changes in size and form of the brain). We use it with its default parameters (see [here](https://nipype.readthedocs.io/en/1.5.0/api/generated/nipype.interfaces.fsl.preprocess.html#fnirt)) in a `nipype` node. `FNIRT` estimates a warp field (i.e. a vector field which describes the displacement of each point). Therefore, it takes quite a long time to run (around 45 minutes on the jupyter hub), so please run it before our session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nipype.interfaces.fsl as fsl\n",
    "from nipype.pipeline.engine import Node\n",
    "import nipype.interfaces.utility as util\n",
    "\n",
    "# initiate workflow\n",
    "nonlinear_reg = Node(fsl.FNIRT(),\n",
    "                name='nonlinear_reg')\n",
    "nonlinear_reg.inputs.in_file = dir_anat + \"T1w_brain.nii.gz\"\n",
    "nonlinear_reg.inputs.ref_file = \"/home/\" + getpass.getuser() +\"/SOC_fMRI_python_workshop/fMRI_data/ICBM_MNI_masked.nii.gz\"\n",
    "nonlinear_reg.inputs.affine_file = dir_anat + \"t1w_2_mni_premat.mat\"\n",
    "nonlinear_reg.inputs.warped_file = dir_anat + \"t1w_2_mni.nii.gz\"\n",
    "nonlinear_reg.inputs.field_file = dir_anat + \"t1w_2_mni_warpfield.nii.gz\"\n",
    "nonlinear_reg_res = nonlinear_reg.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = plotting.plot_anat(\"/home/\" + getpass.getuser() +\"/SOC_fMRI_python_workshop/fMRI_data/ICBM_MNI_masked.nii.gz\", title=\"coregistration T1-MNI\")\n",
    "#display.add_edges()\n",
    "display.add_overlay(dir_anat + \"t1w_2_mni.nii.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember: Our goal is to register our functional motion corrected mean image to the MNI space. Let's plot the functional motion corrected mean image again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_epi(dir_func + \"mcflirt_mean_brain.nii.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we combine both steps and transform our functional image into MNI space with the `ApplyWarp` interface. This function combines and applies different registration matrices and warps. As `premat`, we use the tranform from individual fMRI to anatomical space, and as warp file, we use the transform from anatomical to MNI space. Both transforms taken together yield the (mean) fMRI image in MNI space. It would take too long to transform the complete fMRI scan, therefore we use the mean image here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_func2mni = fsl.ApplyWarp()\n",
    "reg_func2mni.inputs.in_file = dir_func + \"mcflirt_mean_brain.nii.gz\"\n",
    "reg_func2mni.inputs.ref_file = \"/home/\" + getpass.getuser() +\"/SOC_fMRI_python_workshop/fMRI_data/ICBM_MNI_masked.nii.gz\"\n",
    "reg_func2mni.inputs.field_file = dir_anat + \"t1w_2_mni_warpfield.nii.gz\"\n",
    "reg_func2mni.inputs.premat = dir_anat + \"fmri_to_t1w.mat\"\n",
    "reg_func2mni.inputs.out_file = dir_func + \"fmri_to_mni.nii.gz\"\n",
    "res = reg_func2mni.run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = plotting.plot_anat(\"/home/\" + getpass.getuser() +\"/SOC_fMRI_python_workshop/fMRI_data/ICBM_MNI_masked.nii.gz\", title=\"coregistration fMRI-MNI\")\n",
    "display.add_overlay(dir_func + \"fmri_to_mni.nii.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Tissue segmentation\n",
    "For many approaches it is useful to have the data segmented into tissue types. As described in the aMRI lecture, SPM's `Dartel` or `New Segment` are common approaches to do so, which also allow to run voxel-based morphometry.   \n",
    "But `FSL` also provides a tool to perform tissue segmentation. We try this for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_seg = fsl.FAST()\n",
    "fast_seg.inputs.in_files = dir_anat + \"T1w_brain.nii.gz\"\n",
    "fast_seg.inputs.img_type = 1\n",
    "print(fast_seg.cmdline)\n",
    "fast_seg.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from nilearn import plotting\n",
    "display = plotting.plot_anat(dir_anat + \"T1w_brain_pve_0.nii.gz\", title=\"CSF\")\n",
    "display = plotting.plot_anat(dir_anat + \"T1w_brain_pve_1.nii.gz\", title=\"GM\")\n",
    "display = plotting.plot_anat(dir_anat + \"T1w_brain_pve_2.nii.gz\", title=\"WM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interim summary\n",
    "\n",
    "For now, we completed the fundamental preprocessing steps (removing non-brain signal and correcting for spatial distortions). There is probably still substantial noise in the data which we should deal with before the statistical analysis. Therefore, we will now take a look at denoising strategies.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Removing noise confounds\n",
    "Functional MRI signal can be confounded by different non-neuronal noise sources (including slow scanner drift, instrument noise, head motion, respiration, cardiac pulsation). \n",
    "For a detailed review of the possible sources of noise in the BOLD signal, please see [this publication](https://link.springer.com/article/10.1007%2Fs11336-012-9294-0).  \n",
    "In task-based fMRI, the BOLD signal is evaluated in reference to an external stimulus, and thus is less susceptible to the effects of unstructured noise. Yet, noise confounds are usually added to the task regressors in the 1st-level analysis.\n",
    "In resting state fMRI there is no external reference and therefore noise may introduce spurious correlations between brain regions. Here, the most important confounder is head motion, which can have both very pronounced effects (if a participant turns the head to a new position or markably shakes) and very subtle effects (microscopic head movements smaller than the size of a voxel).\n",
    "There are different techniques which aim to reduce the impact of head motion and obtain a cleaner (rs)fMRI signal. \n",
    "\n",
    "Denoising can be grouped into:\n",
    "- **Nuisance regression methods**: include noise as nuisance variables in a linear model (GLM) of the fMRI data to remove their influence (e.g. motion parameters, PCA noise components: [CompCor](https://www.sciencedirect.com/science/article/abs/pii/S1053811907003837), physiological measures)\n",
    "- **Scrubbing methods**: discard disrupted volumes of the data, e.g. described [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3849338/).\n",
    "- **ICA-based methods**: extract signal and noise components of the data and manually/automatically remove noise components ([ICA-AROMA](https://www.sciencedirect.com/science/article/abs/pii/S1053811915001822), [FIX-ICA](https://www.sciencedirect.com/science/article/abs/pii/S1053811914001815))\n",
    "\n",
    "This [paper](https://www.sciencedirect.com/science/article/pii/S1053811917302288) provides a comparison of different strategies to mitigate head motion, and the same authors have published a [protocol](https://experiments.springernature.com/articles/10.1038/s41596-018-0065-y) with detailed instructions how to apply and control head motion correction techniques.\n",
    "This [paper](https://www.sciencedirect.com/science/article/pii/S1053811917310972) also compares the performance of different preprocessing pipelines for resting state fMRI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Visualization of noise confounds\n",
    "First, we have a look at the motion parameters from `mcflirt`. These are three translational and three rotational measures, indicating how much the volume had to be rigidly translated and rotated compared to the reference.  First, we load the data from the `.par` parameter file using `pandas`. This [package]() is very handy for working with text or tabular files in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "realign_params = dir_func + 'mcflirt.nii.gz.par'\n",
    "rp = pd.read_csv(realign_params, header=None, sep='  ')\n",
    "print(np.shape(rp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the motion parameters (red is x, blue is y and green is z-direction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.figure import Figure\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "rp = np.array(rp).T\n",
    "\n",
    "fig = Figure(figsize=(20,20))\n",
    "grid = GridSpec(2, 2) \n",
    "ax = fig.add_subplot(grid[0:1,0])\n",
    "ax.plot(rp[0, :], color=\"red\")\n",
    "ax.plot(rp[1, :], color=\"blue\")\n",
    "ax.plot(rp[2, :], color=\"green\")\n",
    "ax.set_ylabel(\"rotational displacement [rad]\")\n",
    "ax.set_xlabel(\"Frame number\")\n",
    "ax = fig.add_subplot(grid[0:1,1])\n",
    "ax.plot(rp[3, :], color=\"red\")\n",
    "ax.plot(rp[4, :], color=\"blue\")\n",
    "ax.plot(rp[5, :], color=\"green\")\n",
    "ax.set_ylabel(\"translational displacement [mm]\")\n",
    "ax.set_xlabel(\"Frame number\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the rotational parameters, we see that the subject turned its head very subtly to the right throughout the scan (blue rotational parameters indicating rotation around y-axis called roll). This can be best seen when comparing early & late volumes.\n",
    "From the translational parameters, we see that he/she performed up-down movements (green translational parameter indicating movement in the z-direction). There was little displacement in x-direction (red translational parameter), and some displacement in the y-direction in the first half of the scan (blue translational parameter).\n",
    "We can summarize these motion parameters using Framewise displacement, a measure adding the total displacement from frame to frame. See the [original publication](https://pubmed.ncbi.nlm.nih.gov/22019881) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nipype.algorithms.confounds import FramewiseDisplacement\n",
    "\n",
    "FrameDisp = FramewiseDisplacement()\n",
    "FrameDisp.inputs.in_file = realign_params\n",
    "FrameDisp.inputs.out_file = dir_func + \"power_fd.txt\"\n",
    "FrameDisp.inputs.parameter_source = \"FSL\"\n",
    "res_FD = FrameDisp.run()\n",
    "\n",
    "fd = pd.read_csv(FrameDisp.inputs.out_file)\n",
    "print(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_FD.outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compare the motion parameters to the global signal (GS). This is the average of all voxels intensities over time. We compute it with a `nilearn` function which masks the data and returns the masked data in a flat array. Then, we take the average of this array along the second dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.masking import apply_mask\n",
    "import numpy as np\n",
    "\n",
    "masked_data = apply_mask(dir_func + 'mcflirt.nii.gz', \n",
    "                         dir_func + 'mcflirt_mean_brain_mask.nii.gz')\n",
    "GS = np.mean(masked_data,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = Figure(figsize=(10,5))\n",
    "grid = GridSpec(3,1) \n",
    "ax = fig.add_subplot(grid[0,0])\n",
    "ax.plot(rp[3, :])\n",
    "ax.plot(rp[4, :])\n",
    "ax.plot(rp[5, :])\n",
    "ax.set_ylabel(\"translational \\n displacement [mm]\")\n",
    "ax.set_xlabel(\"Frame number\")\n",
    "ax = fig.add_subplot(grid[1,0])\n",
    "ax.plot(fd)\n",
    "ax.set_ylabel(\"FD\")\n",
    "ax.set_xlabel(\"Frame number\")\n",
    "ax = fig.add_subplot(grid[2,0])\n",
    "ax.plot(GS)\n",
    "ax.set_ylabel(\"Global signal\")\n",
    "ax.set_xlabel(\"Frame number\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is a periodic variation in the global signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important diagnostic measure is the temporal derivative of the GS. DVARS is taking the temporal **D**erivative, then calculating the RMS **VAR**iance over element**S**. DVARS is informative about volumes with large signal intensity changes, e.g. due to motion or instrument artifact.  \n",
    "It can be used to identify problematic volumes for later scrubbing or censoring (e.g. excluding the volumes for further analyses or using them as regressors on the first-level).   \n",
    "The `ComputeDVARS` function returns both the unstandardized DVARS (scaled to 1000 leading to the units being expressed in x10 %ŒîBOLD change) and standardized DVARS (normalized with the standard deviation of the temporal difference time series). For more information on which measure and criteria to apply to scrub volumes based on DVARS, please see [this publication](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5915574/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nipype.algorithms.confounds import ComputeDVARS\n",
    "\n",
    "computeDvars = ComputeDVARS()\n",
    "computeDvars.inputs.in_file = dir_func + \"mcflirt.nii.gz\"\n",
    "computeDvars.inputs.in_mask = dir_func + \"mcflirt_mean_brain_mask.nii.gz\"\n",
    "res_DVARS = computeDvars.run() \n",
    "\n",
    "# move the output to the other preprocessing outputs\n",
    "!mv mcflirt_dvars_std.tsv fMRI_data/sub-160563d/func\n",
    "\n",
    "# to have DVARS as plottable object\n",
    "dvars = pd.read_csv(dir_func + \"mcflirt_dvars_std.tsv\", header=None)\n",
    "print(dvars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot all metrics together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = Figure(figsize=(12,7))\n",
    "grid = GridSpec(4,1) \n",
    "ax = fig.add_subplot(grid[0,0])\n",
    "ax.plot(rp[3, :])\n",
    "ax.plot(rp[4, :])\n",
    "ax.plot(rp[5, :])\n",
    "ax.set_ylabel(\"translational \\n displacement [mm]\")\n",
    "ax.set_xlabel(\"Frame number\")\n",
    "ax = fig.add_subplot(grid[1,0])\n",
    "ax.plot(fd)\n",
    "ax.set_ylabel(\"FD\")\n",
    "ax.set_xlabel(\"Frame number\")\n",
    "ax = fig.add_subplot(grid[2,0])\n",
    "ax.plot(GS)\n",
    "ax.set_ylabel(\"Global signal\")\n",
    "ax.set_xlabel(\"Frame number\")\n",
    "ax = fig.add_subplot(grid[3,0])\n",
    "ax.plot(dvars)\n",
    "ax.set_ylabel(\"DVARS\")\n",
    "ax.set_xlabel(\"Frame number\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for some volumes (around frame 180, 250) FD and DVARS peaks coincide. Yet, the global signal does not seem particularly off. This difference is due to the fact that DVARS reflects *variability* of the temporal derivative over voxels, while the global signal is the mean of all voxels. Thus, it is possible that the intensity changes strongly in some voxels, but not at all or only slightly in the majority of all voxels.  \n",
    "To further explore the appearance of confounds in the data, **carpet plots** are extremely valuable. They show the gray values of each voxel (or brain region) for each timepoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_carpet\n",
    "\n",
    "plot_carpet(func, mask_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that actually most voxels show dark bands. This speaks against the low-frequency fluctuations being of neuronal origin, as only GM voxels should show this pattern then. We also see that the global drop in signal around frame 240 (time 480s) is not really related to motion parameters, and may thus be caused by respiration, such as a deep inhale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small excursion about other carpet plots\n",
    "[Carpet plots](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5296400/) combined with head motion parameters, DVARS and if available, physiological data (such as recordings from respiration belts or pulse oxiymetry) are a great tool to investigate confounders in your fMRI data. \n",
    "The carpet plot function is implemented in [fmriprep](https://fmriprep.readthedocs.io/en/stable/usage.html) and [mriqc](https://mriqc.readthedocs.io/en/latest/). See this presentation by [J. Power](https://www.pathlms.com/ohbm/courses/5158/sections/7788/video_presentations/75974), the inventor of *the plot*, on the interpretation.   \n",
    "Below you see three examples from another study, where the carpet shows the signal from voxels in GM (red), WM (green), CSF (orange) & cerebellum (blue). \n",
    "![Example 1](summary_fmriplot_good_example.png)\n",
    "![Example 2](summary_fmriplot_intermediate.png)\n",
    "![Example 3](summary_fmriplot_bad_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More functions to calculate and plot FD and DVARS can be found in the file `motion.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Nuisance regression\n",
    "Including motion parameters (as well as their first and second derivatives) or physiological confounders into resting state and task-based fMRI analyses is commonly accepted. Yet, this is often not **sufficient** (especially for resting state fMRI) to remove unwanted variance. For more information, see these publications which compared different resting state preprocessing strategies by [Parkes](https://www.sciencedirect.com/science/article/pii/S1053811917310972) & [Ciric](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5483393/).  \n",
    "GS regression is an effective way to perform motion correction (as described in the publications above). Yet, it also has the downside of introducing spurious, negative correlations, and introduces a distance-dependence in the connectivity between brain regions.See here for a [consensus statement](https://www.sciencedirect.com/science/article/pii/S1053811916306711) on the usage of GS regression.  \n",
    "Let's create a node which performs a linear regression to remove the six motion parameters and GS from the rs fMRI. A node works in a working directory (`/tmp`), therefore we specify full paths to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 3 rotational and 3 translational motion parameter AND the global signal as regressors\n",
    "\n",
    "# combine motion parameters and GS to one matrix with volumes as rows and nuisance regressors as columns\n",
    "comb = np.hstack((rp.T, np.reshape(GS,(np.shape(rp.T)[0],1))))\n",
    "np.savetxt(dir_func + \"all_regressors.txt\", comb)\n",
    "                     \n",
    "filter2 = Node(fsl.GLM(demean = True,\n",
    "                       out_f_name = dir_func + 'F_noise.nii.gz',\n",
    "                       out_pf_name = dir_func + 'pF_noise.nii.gz',\n",
    "                       out_res_name = dir_func + 'denoised.nii.gz',\n",
    "                       output_type = 'NIFTI_GZ'), \n",
    "                       name='filternoise')\n",
    "filter2.plugin_args = {'submit_specs': 'request_memory = 17000'}\n",
    "filter2.inputs.design = dir_func + 'all_regressors.txt' #./fMRI_data/sub-010088/preproc/all_regressors.txt'\n",
    "filter2.inputs.in_file = dir_func + 'mcflirt.nii.gz'                        \n",
    "filter2.inputs.mask = dir_func + 'mcflirt_mean_brain_mask.nii.gz'\n",
    "res_denoise = filter2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_denoise.outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the functional image and the carpet plot after denoising. The functional image is now centered around 0, that's why it does not contrast with the background. In the carpet plot, we see that the stripy pattern going over all voxels has disappeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image.image import mean_img\n",
    "plot_epi(mean_img(res_denoise.outputs.out_res), cmap=\"gray\")\n",
    "\n",
    "from nilearn.plotting import plot_carpet\n",
    "display = plot_carpet(res_denoise.outputs.out_res, mask_img)\n",
    "display.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the \"new\" GS after the regression to compare it to the \"pre\"-denoised GS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_data_after_regr = apply_mask(dir_func + 'denoised.nii.gz', dir_func + 'mcflirt_mean_brain_mask.nii.gz')\n",
    "print(np.shape(masked_data_after_regr))\n",
    "GS_after_regression = np.mean(masked_data_after_regr,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the undenoised and denoised data to compare individual voxel timecourses. \n",
    "I selected the voxel [32,23,19] (located in the precuneus), but feel free to try out other voxel locations. You can use the interactive viewer to get their coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_func_bef_denoising = nib.load(dir_func + 'mcflirt.nii.gz')\n",
    "img_func_bef_denoising_data = img_func_bef_denoising.get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_func_denoised = nib.load(dir_func + 'denoised.nii.gz')\n",
    "img_func_denoised_data = img_func_denoised.get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.figure import Figure\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.pyplot as plt\n",
    "fig = Figure(figsize=(18,9))\n",
    "grid = GridSpec(7,1) \n",
    "ax = fig.add_subplot(grid[0,0])\n",
    "ax.plot(rp[3])\n",
    "ax.plot(rp[4])\n",
    "ax.plot(rp[5])\n",
    "ax.set_ylabel(\"translational \\n displacement [mm]\")\n",
    "ax.set_xlabel(\"Frame number\")\n",
    "ax = fig.add_subplot(grid[1,0])\n",
    "ax.plot(fd)\n",
    "ax.set_ylabel(\"FD\")\n",
    "ax.set_xlabel(\"Frame number\")\n",
    "ax = fig.add_subplot(grid[2,0])\n",
    "ax.plot(dvars)\n",
    "ax.set_ylabel(\"DVARS\")\n",
    "ax.set_xlabel(\"Frame number\")\n",
    "ax = fig.add_subplot(grid[3,0])\n",
    "ax.plot(GS)\n",
    "ax.set_ylabel(\"GS \\n after motion \\n correction\")\n",
    "ax.set_xlabel(\"Frame number\")\n",
    "ax = fig.add_subplot(grid[4,0])\n",
    "ax.plot(GS_after_regression)\n",
    "ax.set_ylabel(\"GS \\n after regression \\n of confounds\")\n",
    "ax.set_xlabel(\"Frame number\")\n",
    "ax = fig.add_subplot(grid[5,0])\n",
    "ax.plot(img_func_bef_denoising_data[32,23,19,:])\n",
    "ax.set_ylabel(\"Voxel signal \\n after motion \\n correction\")\n",
    "ax.set_xlabel(\"Frame number\")\n",
    "ax = fig.add_subplot(grid[6,0])\n",
    "ax.plot(img_func_denoised_data[32,23,19,:])\n",
    "ax.set_ylabel(\"Voxel signal \\n after regression \\n of confounds\")\n",
    "ax.set_xlabel(\"Frame number\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that after regressing the motion parameters and the global signal, our GS is shifted from around 700 to 0 (compare rows 4 & 5). This is because the denoised data are the **residuals** of the linear regression - and by definition centered around 0. Also, we see that the timeseries is actually *less* smooth - this is becaused we regressed the orange & green timeseries which contain much higher frequency noise.   \n",
    "Also, from rows 6 & 7 we see that the individual voxel timeseries is changed rather subtly by the denoising procedure. Yet, as shown by the carpet plots, the global signal fluctuations (frame number ~ 250) are reduced. To further improve the denoising procedure, we could also include the average signal of the WM or CSF, or temporal derivatives of motion parameters.  \n",
    "##### How to evaluate the denoising performance?\n",
    "This is a very difficult question. For task-based fMRI, one could think of a sanity check (e.g. reliability of model parameters across runs, model performance in general) - but as you explicitely model the activity based on external cue, you are usually on the safe side including the main motion parameters. For resting state fMRI, some hints are given [here](https://experiments.springernature.com/articles/10.1038/s41596-018-0065-y): it is recommended to check the motion - functional connectivity correlation (less is better) and the distance-dependence of the FC (less is better). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look to better understand the denoising outputs. First we plot the pre-and post denoising data for our favorite voxel. We see that they are still highly correlated (which we would expect given the plots above).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(img_func_denoised_data[32,23,19, :], img_func_bef_denoising_data[32,23,19, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation of the timeseries before denoising and the global signal is also very high, indicating a high level of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(img_func_bef_denoising_data[32,23,19, :],GS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6\n",
    "What do you expect (in terms of correlation) if you plot the denoised voxel data against the GS? Please create the plot and explain to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Temporal filtering\n",
    "Our fMRI data still contains periodic components which are more related to noise than to signal (e.g. scanner drifts). Temporal filtering aims to remove such frequencies which are not of interest.  \n",
    "In task-based fMRI, the frequency of interest depends on your design, if e.g. a stimulus is shown every 5 seconds, the frequency of interest would be 0.2 Hz.  \n",
    "In resting state fMRI, we are interested in low-frequency oscillations (i.e. from 0.01 to 0.1 Hz). Frequencies lower than that are often related to scanner drift or coil interference, while higher frequencies might be due to physiological noise (respiration at 0.3 Hz, or heart beat at around 1 Hz). \n",
    "While high-pass filtering is frequently used to remove low-frequency drifts, low-pass or band-pass filtering is more [controversial](https://en.wikibooks.org/wiki/Neuroimaging_Data_Processing/Temporal_Filtering) and sometimes [not recommended](https://neurostars.org/t/bandpass-filtering-different-outputs-from-fsl-and-nipype-custom-function/824).  \n",
    "\n",
    "Temporal filtering can be achieved with different methods, mainly:\n",
    "1. directly apply a gaussian filter to the fMRI data (as implemented in FSL which we use here)\n",
    "2. include regressors for signal drifts in the first-level model (discrete cosine set / linear trends) \n",
    "\n",
    "Here, we use the fsl-interface `TemporalFilter` to do high-pass filtering to our denoised data.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to specifiy the parameter sigma which corresponds to the width of the gaussian filter. Sigma is calculated as a function of the frequency cutoff and the TR. For more information on the gaussian filter, see [here](https://lukas-snoek.com/NI-edu/fMRI-introduction/week_4/temporal_preprocessing.html#high-pass-filtering-of-fmri-data-option-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR = 2\n",
    "HP_freq = 0.01\n",
    "sigma_high = 1 / (2 * TR * HP_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform high-pass filtering to eliminate low-frequency drifts\n",
    "\n",
    "highpass_filter = Node(fsl.TemporalFilter(out_file = dir_func + 'highpass.nii.gz'), name='highpass_filter')\n",
    "highpass_filter.inputs.highpass_sigma = sigma_high\n",
    "highpass_filter.inputs.in_file = dir_func + 'denoised.nii.gz'\n",
    "hp_res = highpass_filter.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compare the signal of a given voxel before denoising, after denoising and after highpass filtering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_func_bef_denoising = nib.load(dir_func + 'mcflirt.nii.gz')\n",
    "img_func_bef_denoising_data = img_func_bef_denoising.get_fdata()\n",
    "img_func_denoised = nib.load(dir_func + 'denoised.nii.gz')\n",
    "img_func_denoised_data = img_func_denoised.get_fdata()\n",
    "img_func_hp = nib.load(dir_func + 'highpass.nii.gz')\n",
    "img_func_hp_data = img_func_hp.get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each signal in a separate subplot\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = Figure(figsize=(18,9))\n",
    "grid = GridSpec(3,1) \n",
    "ax = fig.add_subplot(grid[0,0])\n",
    "ax.plot(img_func_bef_denoising_data[32,23,19, :])\n",
    "ax.set_ylabel(\"Voxel signal \\n after motion \\n correction\")\n",
    "ax.set_xlabel(\"Frame number\")\n",
    "ax = fig.add_subplot(grid[1,0])\n",
    "ax.plot(img_func_denoised_data[32,23,19, :])\n",
    "ax.set_ylabel(\"Voxel signal \\n after regression \\n of confounds\")\n",
    "ax.set_xlabel(\"Frame number\")\n",
    "ax = fig.add_subplot(grid[2,0])\n",
    "ax.plot(img_func_hp_data[32,23,19, :])\n",
    "ax.set_ylabel(\"Voxel signal \\n after regression \\n of confounds & \\n highpass filtering\")\n",
    "ax.set_xlabel(\"Frame number\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see only little difference between the denoised and highpass-filtered time series. This is because much of the low-frequency drift was already removed in the regression step. Yet, there are small differences, e.g. for the first volumes.\n",
    "We can also overlay the signals before and after highpass filtering to see the subtle differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay signals before and after highpass filtering\n",
    "plt.plot(img_func_denoised_data[32,23,19, :])\n",
    "plt.plot(img_func_hp_data[32,23,19, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Exercise\n",
    "Run the high-pass filter on the motion corrected data before denoising! Overlay the signal before and after high-pass filtering (note: different scaling of the data requires normalization)!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Smoothing\n",
    "Spatially smoothing your fMRI has advantages for detecting differences in group-analysis: it improves the signal-to-noise ratio and accounts for local activation differences between individual participants. Usually, you use a Gaussian filter with a kernel size around 4-10 mm. Ideally, filter width matches the expected signal width (fMRI data is inherently smooth because of co-activation and hemodynamic convolution). See also this [page](https://support.brainvoyager.com/brainvoyager/functional-analysis-preparation/29-pre-processing/86-spatial-smoothing) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import IsotropicSmooth from the FSL interface\n",
    "from nipype.interfaces.fsl import IsotropicSmooth\n",
    "\n",
    "# Define a node\n",
    "smooth_node = Node(IsotropicSmooth(), name=\"smoothing\")\n",
    "smooth_node.inputs.in_file = dir_func + \"highpass.nii.gz\"\n",
    "#smooth_node.inputs.in_file = dir_func + \"mcflirt.nii.gz\"\n",
    "smooth_node.inputs.fwhm = 4\n",
    "smooth_node.inputs.out_file = dir_func + \"highpass_4mm.nii.gz\"\n",
    "smooth_res = smooth_node.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_epi\n",
    "from nilearn.image.image import mean_img\n",
    "\n",
    "plot_epi(mean_img(smooth_node.inputs.in_file), cmap=\"gray\")\n",
    "plot_epi(mean_img(smooth_node.inputs.out_file),  cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "As the denoised image is already demeaned, this plot has a low contrast and therefore doesn't show the smoothing effect very well. For a better comparison, use a non-denoised image (e.g. data after motion correction) as input (see commented line above). Try smoothing with 6mm and 10mm kernels and compare the results visually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Denoising within the first-level model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous steps to remove noise can also be incorporated directly in a linear model of the fMRI timeseries (first-level model). We already performed a nuisance regression [above](#4.2-Nuisance-regression) using FSL. This can also be implemented in nilearn. For example, noise regressors can be added to the design matrix of the first-level model and arguments can be used to specify high-pass filtering and smoothing options.\n",
    "\n",
    "In task-based fMRI, the first-level model is used to investigate effects of interest as designed by the task, e.g. task parameters as beta values ([here](\n",
    "https://nilearn.github.io/stable/auto_examples/plot_single_subject_single_run.html#sphx-glr-auto-examples-plot-single-subject-single-run-py) an example. With our resting-state dataset here, we can use nilearn to remove noise from the data via nuisance regression and use the residuals for further analyses. Below we run a first-level model with noise regressors and extract the residuals and explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.glm.first_level import FirstLevelModel, make_first_level_design_matrix\n",
    "from nilearn.plotting import plot_design_matrix\n",
    "\n",
    "# set arguments of function FirstLevelModel, including high-pass filtering and smoothing\n",
    "glm = FirstLevelModel(\n",
    "    t_r = 2,\n",
    "    hrf_model = 'glover',\n",
    "    drift_model = 'cosine',\n",
    "    high_pass = 0.01,\n",
    "    smoothing_fwhm = 4,\n",
    "    minimize_memory = False,\n",
    "    n_jobs = -1)\n",
    "\n",
    "# create design matrix which includes the realignment parameters\n",
    "realign_param = pd.read_csv(dir_func + \"mcflirt.nii.gz.par\", header=None, sep='  ')\n",
    "design_matrix = make_first_level_design_matrix(frame_times = np.arange(img_func.shape[3])*2,\n",
    "                                    add_regs = realign_param)\n",
    "\n",
    "# run the glm\n",
    "glm.fit(run_imgs = dir_func + 'mcflirt.nii.gz',\n",
    "        events = None,\n",
    "        confounds = None,\n",
    "        design_matrices = design_matrix)\n",
    "\n",
    "print(\"Plotting the design matrix\")\n",
    "plot_design_matrix(glm.design_matrices_[0])\n",
    "plt.show()\n",
    "\n",
    "# get the denoised image = residuals\n",
    "img_denoised = glm.residuals[0]\n",
    "\n",
    "# get the explained variance\n",
    "img_rsquare = glm.r_square[0]\n",
    "explained_variance = nilearn.masking.apply_mask(img_rsquare, mask_img).mean()\n",
    "print(f'Across the brain, the regressors explain {explained_variance.round(2) * 100} % of the variance of the data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardized preprocessing pipelines: fMRIPrep and MRIQC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned at the beginning of this notebook, fMRI preprocessing can be performed almost automatically by standardized pipelines such as [fMRIPrep](https://fmriprep.org/en/stable/). fMRIPrep combines the nipype interfaces we used before with preprocessing functions of various fMRI software packages (e.g. FSL, SPM, Freesurfer). Here's an overview of fMRIPrep's workflow:\n",
    "\n",
    "![picture](https://github.com/oesteban/fmriprep/raw/f4c7a9804be26c912b24ef4dccba54bdd72fa1fd/docs/_static/fmriprep-21.0.0.svg)\n",
    "\n",
    "fMRIPrep makes use of a standardized data structure called [BIDS](https://bids.neuroimaging.io/). Once the data (e.g. MRI images, behavioral (event) files) is organized in BIDS, fMRIPrep can be easily executed by a command. In the easiest case (without specifying further parameters) a command would look like:  \n",
    "`fmriprep data/bids_root/ out/ participant -w work/`\n",
    "\n",
    "In addition to the preprocessed brain images and estimated confounds, fMRIPrep also generates a report with information on the individual steps including e.g. registration and motion. An example report can be found [here](https://fmriprep.org/en/stable/_static/SampleReport/sample_report.html).\n",
    "\n",
    "\n",
    "More quality checks on both functional and structural MRI images can be performed with [MRIQC](https://mriqc.readthedocs.io/en/stable/). MRIQC computes a variety of quality metrics, e.g. motion, signal-to-noise ratio, image artifacts. \n",
    "\n",
    "![picture](https://mriqc.readthedocs.io/en/stable/_images/OHBM2017-poster.png)\n",
    "\n",
    "Similar to fMRIPrep, MRIQC makes use of the BIDS format and can be executed by a command - in the easiest case:  \n",
    "`mriqc bids-root/ output-folder/ participant`\n",
    "\n",
    "[Here](https://mriqc.readthedocs.io/en/stable/reports.html)'s a short demonstration of MRIQC and [example report](http://web.stanford.edu/group/poldracklab/mriqc/reports/sub-50013_task-rest_bold.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you for participating in this workshop!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
